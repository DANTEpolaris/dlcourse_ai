{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W1\n",
      "Gradient check passed!\n",
      "Checking gradient for W2\n",
      "Gradient check passed!\n",
      "Checking gradient for B1\n",
      "Gradient check passed!\n",
      "Checking gradient for B2\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W1\n",
      "Gradient check passed!\n",
      "Checking gradient for W2\n",
      "Gradient check passed!\n",
      "Checking gradient for B1\n",
      "Gradient check passed!\n",
      "Checking gradient for B2\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09999999999999998"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.937383, Train accuracy: 0.228222, val accuracy: 0.241000\n",
      "Loss: 1.691970, Train accuracy: 0.434000, val accuracy: 0.435000\n",
      "Loss: 1.207969, Train accuracy: 0.531111, val accuracy: 0.546000\n",
      "Loss: 1.253839, Train accuracy: 0.643667, val accuracy: 0.623000\n",
      "Loss: 0.990129, Train accuracy: 0.683444, val accuracy: 0.647000\n",
      "Loss: 0.516938, Train accuracy: 0.717889, val accuracy: 0.673000\n",
      "Loss: 0.708548, Train accuracy: 0.724889, val accuracy: 0.680000\n",
      "Loss: 0.559733, Train accuracy: 0.753111, val accuracy: 0.703000\n",
      "Loss: 0.998737, Train accuracy: 0.758556, val accuracy: 0.708000\n",
      "Loss: 0.427538, Train accuracy: 0.764111, val accuracy: 0.698000\n",
      "Loss: 1.054560, Train accuracy: 0.767222, val accuracy: 0.707000\n",
      "Loss: 0.733797, Train accuracy: 0.785778, val accuracy: 0.709000\n",
      "Loss: 0.686326, Train accuracy: 0.807222, val accuracy: 0.725000\n",
      "Loss: 0.602962, Train accuracy: 0.811778, val accuracy: 0.726000\n",
      "Loss: 0.853548, Train accuracy: 0.806556, val accuracy: 0.720000\n",
      "Loss: 0.515570, Train accuracy: 0.808333, val accuracy: 0.710000\n",
      "Loss: 0.786611, Train accuracy: 0.842000, val accuracy: 0.738000\n",
      "Loss: 1.003634, Train accuracy: 0.830111, val accuracy: 0.725000\n",
      "Loss: 0.688869, Train accuracy: 0.849222, val accuracy: 0.730000\n",
      "Loss: 1.168913, Train accuracy: 0.864556, val accuracy: 0.735000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-5)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down and train and val accuracy go up for every epoch\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1aa72474438>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.991167, Train accuracy: 0.297778, val accuracy: 0.309000\n",
      "Loss: 1.661923, Train accuracy: 0.576222, val accuracy: 0.563000\n",
      "Loss: 1.889620, Train accuracy: 0.615889, val accuracy: 0.588000\n",
      "Loss: 0.807743, Train accuracy: 0.665556, val accuracy: 0.650000\n",
      "Loss: 1.163531, Train accuracy: 0.704889, val accuracy: 0.659000\n",
      "Loss: 1.848948, Train accuracy: 0.723667, val accuracy: 0.682000\n",
      "Loss: 0.722320, Train accuracy: 0.735111, val accuracy: 0.694000\n",
      "Loss: 0.830592, Train accuracy: 0.768444, val accuracy: 0.700000\n",
      "Loss: 1.385704, Train accuracy: 0.740333, val accuracy: 0.682000\n",
      "Loss: 0.548935, Train accuracy: 0.796000, val accuracy: 0.719000\n",
      "Loss: 0.627751, Train accuracy: 0.803889, val accuracy: 0.708000\n",
      "Loss: 0.348919, Train accuracy: 0.819222, val accuracy: 0.725000\n",
      "Loss: 0.912425, Train accuracy: 0.810333, val accuracy: 0.704000\n",
      "Loss: 0.449253, Train accuracy: 0.812778, val accuracy: 0.710000\n",
      "Loss: 0.923551, Train accuracy: 0.791000, val accuracy: 0.705000\n",
      "Loss: 0.649664, Train accuracy: 0.793667, val accuracy: 0.689000\n",
      "Loss: 0.565465, Train accuracy: 0.833667, val accuracy: 0.723000\n",
      "Loss: 0.477759, Train accuracy: 0.850667, val accuracy: 0.714000\n",
      "Loss: 0.569946, Train accuracy: 0.842222, val accuracy: 0.703000\n",
      "Loss: 0.431602, Train accuracy: 0.879556, val accuracy: 0.732000\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-5)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=15e-2, learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.088906, Train accuracy: 0.201000, val accuracy: 0.210000\n",
      "Loss: 1.725543, Train accuracy: 0.368111, val accuracy: 0.373000\n",
      "Loss: 1.253919, Train accuracy: 0.564333, val accuracy: 0.553000\n",
      "Loss: 1.224864, Train accuracy: 0.635000, val accuracy: 0.617000\n",
      "Loss: 1.168459, Train accuracy: 0.679889, val accuracy: 0.653000\n",
      "Loss: 1.651818, Train accuracy: 0.690000, val accuracy: 0.663000\n",
      "Loss: 0.955525, Train accuracy: 0.736111, val accuracy: 0.693000\n",
      "Loss: 1.009603, Train accuracy: 0.720222, val accuracy: 0.670000\n",
      "Loss: 0.764414, Train accuracy: 0.766667, val accuracy: 0.705000\n",
      "Loss: 0.958216, Train accuracy: 0.779556, val accuracy: 0.706000\n",
      "Loss: 0.543668, Train accuracy: 0.805000, val accuracy: 0.721000\n",
      "Loss: 0.572376, Train accuracy: 0.801667, val accuracy: 0.718000\n",
      "Loss: 0.403788, Train accuracy: 0.825222, val accuracy: 0.711000\n",
      "Loss: 0.355430, Train accuracy: 0.824778, val accuracy: 0.716000\n",
      "Loss: 0.961970, Train accuracy: 0.850556, val accuracy: 0.740000\n",
      "Loss: 0.659453, Train accuracy: 0.835111, val accuracy: 0.736000\n",
      "Loss: 0.649940, Train accuracy: 0.844444, val accuracy: 0.725000\n",
      "Loss: 0.692535, Train accuracy: 0.850556, val accuracy: 0.741000\n",
      "Loss: 0.264899, Train accuracy: 0.859000, val accuracy: 0.728000\n",
      "Loss: 0.217131, Train accuracy: 0.870333, val accuracy: 0.732000\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-5)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-2, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.331536, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.327809, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: 2.324410, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.321467, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.317159, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.314732, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.298972, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.265042, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.164654, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.026035, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.928404, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.276459, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.694029, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.279258, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.980433, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.671532, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.072443, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.404688, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.510229, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.802216, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.001019, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.564169, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.000170, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.861683, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.284141, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.091501, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.756062, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.170402, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.028377, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.337819, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.811381, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.655966, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.423664, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.967618, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.688980, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 2.051869, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.096052, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.644434, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.736020, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.928440, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 2.498896, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.142952, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.712883, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.744973, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.462830, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.926650, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.384994, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.641820, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.690040, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.363534, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.594940, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.640557, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.596898, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.899764, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.599870, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.662263, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.083183, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.012170, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.564296, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.004633, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 2.094925, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.671883, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.371514, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.370925, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 2.010186, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.322573, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.282410, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 0.923735, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.191248, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.785722, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.649640, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.236244, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.596470, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.763747, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.289784, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.812939, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 2.055681, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.393128, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 0.980029, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.645135, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.660953, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.165817, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.309539, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.576389, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 0.953137, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.274374, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.310576, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 0.939462, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.379864, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.708357, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.394707, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.632183, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.567198, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.218064, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.316126, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.295034, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.051334, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.721393, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.277583, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.587643, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.887955, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.329934, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.886486, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.161714, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.336610, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.280655, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.444234, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.587572, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.260126, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.258582, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.561379, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.275210, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.102041, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.341083, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.383627, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.426870, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 1.497271, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.363796, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.599793, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.668012, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.241540, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.712235, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.150542, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.628581, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.347210, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.162810, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.205019, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.115407, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.014869, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.673745, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.645457, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.412371, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.209850, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.195837, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.275738, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.391908, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.238971, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.435165, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.385543, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.107614, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.481251, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.406406, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.218750, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.483962, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.339460, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.480786, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.319428, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.372459, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.491327, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.462726, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.302374, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.300644, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.295383, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.018488, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.079099, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: 1.829370, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.958315, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 2.619223, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.536753, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.708987, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 0.695480, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.936009, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.327485, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 0.478932, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 0.483440, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 0.677020, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 0.768356, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 0.080648, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 0.333316, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.103902, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-5)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=3e-1, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **40%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.021700, Train accuracy: 0.287778, val accuracy: 0.292000\n",
      "Loss: 1.478954, Train accuracy: 0.460889, val accuracy: 0.451000\n",
      "Loss: 1.800945, Train accuracy: 0.531444, val accuracy: 0.501000\n",
      "Loss: 1.566881, Train accuracy: 0.551889, val accuracy: 0.525000\n",
      "Loss: 1.295172, Train accuracy: 0.648333, val accuracy: 0.644000\n",
      "Loss: 1.169863, Train accuracy: 0.601000, val accuracy: 0.581000\n",
      "Loss: 1.409411, Train accuracy: 0.509778, val accuracy: 0.511000\n",
      "Loss: 1.478483, Train accuracy: 0.657222, val accuracy: 0.637000\n",
      "Loss: 1.412554, Train accuracy: 0.591333, val accuracy: 0.563000\n",
      "Loss: 3.141732, Train accuracy: 0.490222, val accuracy: 0.462000\n",
      "Loss: 1.343070, Train accuracy: 0.704111, val accuracy: 0.654000\n",
      "Loss: 2.460622, Train accuracy: 0.407667, val accuracy: 0.408000\n",
      "Loss: 1.173145, Train accuracy: 0.717222, val accuracy: 0.652000\n",
      "Loss: 1.291142, Train accuracy: 0.724222, val accuracy: 0.667000\n",
      "Loss: 1.868806, Train accuracy: 0.523000, val accuracy: 0.493000\n",
      "Loss: 2.002052, Train accuracy: 0.564222, val accuracy: 0.531000\n",
      "Loss: 1.349868, Train accuracy: 0.706778, val accuracy: 0.662000\n",
      "Loss: 1.548058, Train accuracy: 0.670889, val accuracy: 0.639000\n",
      "Loss: 1.719611, Train accuracy: 0.693222, val accuracy: 0.635000\n",
      "Loss: 1.388312, Train accuracy: 0.742000, val accuracy: 0.668000\n",
      "Loss: 1.436819, Train accuracy: 0.608222, val accuracy: 0.572000\n",
      "Loss: 2.980865, Train accuracy: 0.520889, val accuracy: 0.525000\n",
      "Loss: 2.239389, Train accuracy: 0.575889, val accuracy: 0.549000\n",
      "Loss: 1.232460, Train accuracy: 0.736111, val accuracy: 0.681000\n",
      "Loss: 2.184166, Train accuracy: 0.655111, val accuracy: 0.598000\n",
      "Loss: 1.282682, Train accuracy: 0.711778, val accuracy: 0.667000\n",
      "Loss: 1.295697, Train accuracy: 0.694889, val accuracy: 0.645000\n",
      "Loss: 1.484175, Train accuracy: 0.636000, val accuracy: 0.603000\n",
      "Loss: 1.220649, Train accuracy: 0.779889, val accuracy: 0.689000\n",
      "Loss: 0.799908, Train accuracy: 0.809889, val accuracy: 0.726000\n",
      "Loss: 1.078350, Train accuracy: 0.728556, val accuracy: 0.674000\n",
      "Loss: 1.246761, Train accuracy: 0.749667, val accuracy: 0.673000\n",
      "Loss: 0.879268, Train accuracy: 0.797333, val accuracy: 0.724000\n",
      "Loss: 3.291126, Train accuracy: 0.466444, val accuracy: 0.434000\n",
      "Loss: 1.026166, Train accuracy: 0.777667, val accuracy: 0.712000\n",
      "Loss: 2.765066, Train accuracy: 0.581222, val accuracy: 0.554000\n",
      "Loss: 1.156482, Train accuracy: 0.753000, val accuracy: 0.670000\n",
      "Loss: 1.472179, Train accuracy: 0.755111, val accuracy: 0.680000\n",
      "Loss: 1.135630, Train accuracy: 0.766889, val accuracy: 0.707000\n",
      "Loss: 1.013421, Train accuracy: 0.802222, val accuracy: 0.708000\n",
      "Loss: 1.073554, Train accuracy: 0.737222, val accuracy: 0.666000\n",
      "Loss: 1.184650, Train accuracy: 0.802222, val accuracy: 0.732000\n",
      "Loss: 0.997333, Train accuracy: 0.767667, val accuracy: 0.679000\n",
      "Loss: 0.980058, Train accuracy: 0.837444, val accuracy: 0.741000\n",
      "Loss: 1.004174, Train accuracy: 0.784778, val accuracy: 0.705000\n",
      "Loss: 1.371028, Train accuracy: 0.745222, val accuracy: 0.681000\n",
      "Loss: 0.836801, Train accuracy: 0.819889, val accuracy: 0.737000\n",
      "Loss: 1.248065, Train accuracy: 0.798000, val accuracy: 0.712000\n",
      "Loss: 1.104788, Train accuracy: 0.742667, val accuracy: 0.669000\n",
      "Loss: 0.828332, Train accuracy: 0.819889, val accuracy: 0.729000\n",
      "Loss: 0.997502, Train accuracy: 0.766222, val accuracy: 0.693000\n",
      "Loss: 1.011680, Train accuracy: 0.786222, val accuracy: 0.700000\n",
      "Loss: 1.557664, Train accuracy: 0.707111, val accuracy: 0.657000\n",
      "Loss: 0.630845, Train accuracy: 0.779444, val accuracy: 0.693000\n",
      "Loss: 1.259987, Train accuracy: 0.678444, val accuracy: 0.624000\n",
      "Loss: 1.356981, Train accuracy: 0.728222, val accuracy: 0.651000\n",
      "Loss: 1.064936, Train accuracy: 0.814333, val accuracy: 0.720000\n",
      "Loss: 1.055268, Train accuracy: 0.807111, val accuracy: 0.722000\n",
      "Loss: 0.919120, Train accuracy: 0.839222, val accuracy: 0.743000\n",
      "Loss: 1.324354, Train accuracy: 0.667222, val accuracy: 0.613000\n",
      "Loss: 1.115811, Train accuracy: 0.721333, val accuracy: 0.652000\n",
      "Loss: 2.137320, Train accuracy: 0.730778, val accuracy: 0.655000\n",
      "Loss: 1.214211, Train accuracy: 0.793444, val accuracy: 0.709000\n",
      "Loss: 1.102290, Train accuracy: 0.800778, val accuracy: 0.712000\n",
      "Loss: 1.241878, Train accuracy: 0.781889, val accuracy: 0.697000\n",
      "Loss: 0.948847, Train accuracy: 0.863556, val accuracy: 0.759000\n",
      "Loss: 1.079863, Train accuracy: 0.773000, val accuracy: 0.685000\n",
      "Loss: 1.022784, Train accuracy: 0.702889, val accuracy: 0.656000\n",
      "Loss: 0.980485, Train accuracy: 0.817222, val accuracy: 0.711000\n",
      "Loss: 0.867272, Train accuracy: 0.800000, val accuracy: 0.704000\n",
      "Loss: 0.611916, Train accuracy: 0.858333, val accuracy: 0.750000\n",
      "Loss: 1.650516, Train accuracy: 0.715111, val accuracy: 0.637000\n",
      "Loss: 0.966394, Train accuracy: 0.866111, val accuracy: 0.763000\n",
      "Loss: 0.784799, Train accuracy: 0.820667, val accuracy: 0.725000\n",
      "Loss: 1.534123, Train accuracy: 0.672556, val accuracy: 0.614000\n",
      "Loss: 0.945393, Train accuracy: 0.863111, val accuracy: 0.753000\n",
      "Loss: 0.982100, Train accuracy: 0.801889, val accuracy: 0.709000\n",
      "Loss: 1.172975, Train accuracy: 0.838333, val accuracy: 0.728000\n",
      "Loss: 0.959791, Train accuracy: 0.881667, val accuracy: 0.769000\n",
      "Loss: 1.938579, Train accuracy: 0.768000, val accuracy: 0.694000\n",
      "Loss: 0.894646, Train accuracy: 0.865444, val accuracy: 0.766000\n",
      "Loss: 0.910661, Train accuracy: 0.850222, val accuracy: 0.736000\n",
      "Loss: 0.889124, Train accuracy: 0.866000, val accuracy: 0.762000\n",
      "Loss: 0.830033, Train accuracy: 0.828111, val accuracy: 0.712000\n",
      "Loss: 0.868055, Train accuracy: 0.841778, val accuracy: 0.722000\n",
      "Loss: 0.864295, Train accuracy: 0.840222, val accuracy: 0.728000\n",
      "Loss: 0.903450, Train accuracy: 0.879111, val accuracy: 0.763000\n",
      "Loss: 0.852863, Train accuracy: 0.874556, val accuracy: 0.766000\n",
      "Loss: 0.721668, Train accuracy: 0.875000, val accuracy: 0.755000\n",
      "Loss: 0.631691, Train accuracy: 0.880778, val accuracy: 0.771000\n",
      "Loss: 1.804589, Train accuracy: 0.704667, val accuracy: 0.639000\n",
      "Loss: 1.171074, Train accuracy: 0.793222, val accuracy: 0.692000\n",
      "Loss: 1.129231, Train accuracy: 0.853444, val accuracy: 0.750000\n",
      "Loss: 1.128605, Train accuracy: 0.827778, val accuracy: 0.726000\n",
      "Loss: 0.887444, Train accuracy: 0.862667, val accuracy: 0.748000\n",
      "Loss: 0.967562, Train accuracy: 0.866333, val accuracy: 0.753000\n",
      "Loss: 1.026407, Train accuracy: 0.810000, val accuracy: 0.721000\n",
      "Loss: 0.852707, Train accuracy: 0.879889, val accuracy: 0.756000\n",
      "Loss: 0.772241, Train accuracy: 0.850667, val accuracy: 0.742000\n",
      "Loss: 0.883290, Train accuracy: 0.875333, val accuracy: 0.750000\n",
      "Loss: 0.935555, Train accuracy: 0.870667, val accuracy: 0.753000\n",
      "Loss: 2.370274, Train accuracy: 0.550556, val accuracy: 0.499000\n",
      "Loss: 0.750957, Train accuracy: 0.876556, val accuracy: 0.741000\n",
      "Loss: 0.953731, Train accuracy: 0.868667, val accuracy: 0.740000\n",
      "Loss: 1.068742, Train accuracy: 0.860333, val accuracy: 0.746000\n",
      "Loss: 0.844540, Train accuracy: 0.853222, val accuracy: 0.743000\n",
      "Loss: 1.080544, Train accuracy: 0.747222, val accuracy: 0.667000\n",
      "Loss: 0.853277, Train accuracy: 0.850444, val accuracy: 0.733000\n",
      "Loss: 1.282534, Train accuracy: 0.752778, val accuracy: 0.659000\n",
      "Loss: 1.029955, Train accuracy: 0.798889, val accuracy: 0.702000\n",
      "Loss: 0.664990, Train accuracy: 0.894889, val accuracy: 0.766000\n",
      "Loss: 0.930353, Train accuracy: 0.903444, val accuracy: 0.775000\n",
      "Loss: 0.588853, Train accuracy: 0.906556, val accuracy: 0.769000\n",
      "Loss: 0.843578, Train accuracy: 0.892778, val accuracy: 0.769000\n",
      "Loss: 0.984315, Train accuracy: 0.834000, val accuracy: 0.724000\n",
      "Loss: 0.767144, Train accuracy: 0.857111, val accuracy: 0.732000\n",
      "Loss: 1.007894, Train accuracy: 0.890000, val accuracy: 0.749000\n",
      "Loss: 0.748429, Train accuracy: 0.898333, val accuracy: 0.768000\n",
      "Loss: 0.704595, Train accuracy: 0.896556, val accuracy: 0.766000\n",
      "Loss: 0.956548, Train accuracy: 0.894778, val accuracy: 0.752000\n",
      "Loss: 1.092707, Train accuracy: 0.820222, val accuracy: 0.722000\n",
      "Loss: 0.905048, Train accuracy: 0.882111, val accuracy: 0.766000\n",
      "Loss: 0.813552, Train accuracy: 0.832667, val accuracy: 0.716000\n",
      "Loss: 1.014822, Train accuracy: 0.768889, val accuracy: 0.674000\n",
      "Loss: 0.713762, Train accuracy: 0.894556, val accuracy: 0.768000\n",
      "Loss: 0.636409, Train accuracy: 0.909778, val accuracy: 0.776000\n",
      "Loss: 0.739723, Train accuracy: 0.907778, val accuracy: 0.767000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.045145, Train accuracy: 0.729778, val accuracy: 0.641000\n",
      "Loss: 0.861445, Train accuracy: 0.888333, val accuracy: 0.753000\n",
      "Loss: 0.720663, Train accuracy: 0.884111, val accuracy: 0.749000\n",
      "Loss: 0.858815, Train accuracy: 0.905667, val accuracy: 0.766000\n",
      "Loss: 0.949365, Train accuracy: 0.854889, val accuracy: 0.728000\n",
      "Loss: 1.013948, Train accuracy: 0.910000, val accuracy: 0.765000\n",
      "Loss: 0.726394, Train accuracy: 0.878778, val accuracy: 0.744000\n",
      "Loss: 0.824586, Train accuracy: 0.866778, val accuracy: 0.742000\n",
      "Loss: 0.992408, Train accuracy: 0.861556, val accuracy: 0.727000\n",
      "Loss: 0.665165, Train accuracy: 0.918111, val accuracy: 0.780000\n",
      "Loss: 0.705856, Train accuracy: 0.915778, val accuracy: 0.776000\n",
      "Loss: 0.825402, Train accuracy: 0.908333, val accuracy: 0.762000\n",
      "Loss: 0.704087, Train accuracy: 0.914444, val accuracy: 0.775000\n",
      "Loss: 0.682349, Train accuracy: 0.920000, val accuracy: 0.769000\n",
      "Loss: 0.697035, Train accuracy: 0.911444, val accuracy: 0.781000\n",
      "Loss: 0.802646, Train accuracy: 0.891333, val accuracy: 0.753000\n",
      "Loss: 0.885395, Train accuracy: 0.838444, val accuracy: 0.723000\n",
      "Loss: 0.769349, Train accuracy: 0.928889, val accuracy: 0.772000\n",
      "Loss: 0.663064, Train accuracy: 0.920556, val accuracy: 0.765000\n",
      "Loss: 0.752460, Train accuracy: 0.906444, val accuracy: 0.763000\n",
      "Loss: 0.877304, Train accuracy: 0.888333, val accuracy: 0.744000\n",
      "Loss: 0.629761, Train accuracy: 0.922111, val accuracy: 0.775000\n",
      "Loss: 0.724806, Train accuracy: 0.917111, val accuracy: 0.767000\n",
      "Loss: 0.767987, Train accuracy: 0.900111, val accuracy: 0.752000\n",
      "Loss: 0.658817, Train accuracy: 0.915333, val accuracy: 0.775000\n",
      "Loss: 0.800796, Train accuracy: 0.907111, val accuracy: 0.757000\n",
      "Loss: 0.653193, Train accuracy: 0.936222, val accuracy: 0.790000\n",
      "Loss: 0.792553, Train accuracy: 0.918667, val accuracy: 0.784000\n",
      "Loss: 0.853143, Train accuracy: 0.905222, val accuracy: 0.763000\n",
      "Loss: 0.768738, Train accuracy: 0.882333, val accuracy: 0.760000\n",
      "Loss: 0.808735, Train accuracy: 0.844556, val accuracy: 0.719000\n",
      "Loss: 0.862732, Train accuracy: 0.929222, val accuracy: 0.780000\n",
      "Loss: 0.601134, Train accuracy: 0.921889, val accuracy: 0.783000\n",
      "Loss: 0.658191, Train accuracy: 0.918778, val accuracy: 0.777000\n",
      "Loss: 0.865985, Train accuracy: 0.909444, val accuracy: 0.760000\n",
      "Loss: 0.659003, Train accuracy: 0.913000, val accuracy: 0.774000\n",
      "Loss: 0.794521, Train accuracy: 0.924222, val accuracy: 0.775000\n",
      "Loss: 0.854444, Train accuracy: 0.930000, val accuracy: 0.787000\n",
      "Loss: 0.680867, Train accuracy: 0.921000, val accuracy: 0.771000\n",
      "Loss: 0.740263, Train accuracy: 0.912889, val accuracy: 0.775000\n",
      "Loss: 0.718446, Train accuracy: 0.927000, val accuracy: 0.783000\n",
      "Loss: 0.687668, Train accuracy: 0.923889, val accuracy: 0.783000\n",
      "Loss: 0.702064, Train accuracy: 0.896000, val accuracy: 0.753000\n",
      "Loss: 0.735545, Train accuracy: 0.875667, val accuracy: 0.744000\n",
      "Loss: 0.717432, Train accuracy: 0.921444, val accuracy: 0.773000\n",
      "Loss: 0.797457, Train accuracy: 0.934778, val accuracy: 0.785000\n",
      "Loss: 0.619975, Train accuracy: 0.943778, val accuracy: 0.786000\n",
      "Loss: 0.654462, Train accuracy: 0.930667, val accuracy: 0.777000\n",
      "Loss: 0.796022, Train accuracy: 0.929000, val accuracy: 0.782000\n",
      "Loss: 0.671945, Train accuracy: 0.903778, val accuracy: 0.760000\n",
      "Loss: 0.721430, Train accuracy: 0.940667, val accuracy: 0.790000\n",
      "Loss: 0.792109, Train accuracy: 0.935667, val accuracy: 0.786000\n",
      "Loss: 0.754383, Train accuracy: 0.921000, val accuracy: 0.776000\n",
      "Loss: 0.773107, Train accuracy: 0.902222, val accuracy: 0.764000\n",
      "Loss: 0.773107, Train accuracy: 0.925333, val accuracy: 0.782000\n",
      "Loss: 0.711454, Train accuracy: 0.928333, val accuracy: 0.780000\n",
      "Loss: 0.751538, Train accuracy: 0.940889, val accuracy: 0.779000\n",
      "Loss: 0.643626, Train accuracy: 0.944444, val accuracy: 0.787000\n",
      "Loss: 0.615627, Train accuracy: 0.915222, val accuracy: 0.762000\n",
      "Loss: 0.632077, Train accuracy: 0.933556, val accuracy: 0.787000\n",
      "Loss: 0.659340, Train accuracy: 0.938889, val accuracy: 0.788000\n",
      "Loss: 0.730995, Train accuracy: 0.878000, val accuracy: 0.741000\n",
      "Loss: 0.637106, Train accuracy: 0.947111, val accuracy: 0.792000\n",
      "Loss: 0.581968, Train accuracy: 0.951667, val accuracy: 0.795000\n",
      "Loss: 0.687705, Train accuracy: 0.936000, val accuracy: 0.784000\n",
      "Loss: 0.695940, Train accuracy: 0.948778, val accuracy: 0.795000\n",
      "Loss: 0.709989, Train accuracy: 0.936556, val accuracy: 0.780000\n",
      "Loss: 0.561567, Train accuracy: 0.937556, val accuracy: 0.778000\n",
      "Loss: 0.776531, Train accuracy: 0.936000, val accuracy: 0.783000\n",
      "Loss: 0.649818, Train accuracy: 0.946333, val accuracy: 0.788000\n",
      "Loss: 0.680019, Train accuracy: 0.945444, val accuracy: 0.783000\n",
      "Loss: 0.771225, Train accuracy: 0.942333, val accuracy: 0.787000\n",
      "Loss: 0.629921, Train accuracy: 0.943667, val accuracy: 0.788000\n",
      "Params: lr=0.5, rs=0.001, opt=<class 'optim.SGD'>\n",
      "Loss: 0.6824, Train accuracy: 0.9437, val accuracy: 0.7880\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\src\\DL\\assignments\\assignment2\\layers.py:62: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.mean(-np.log(probs[np.arange(n_samples), target_index]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: inf, Train accuracy: 0.079000, val accuracy: 0.079000\n",
      "Loss: inf, Train accuracy: 0.109556, val accuracy: 0.101000\n",
      "Loss: inf, Train accuracy: 0.118111, val accuracy: 0.116000\n",
      "Loss: inf, Train accuracy: 0.098333, val accuracy: 0.092000\n",
      "Loss: inf, Train accuracy: 0.105667, val accuracy: 0.089000\n",
      "Loss: inf, Train accuracy: 0.106000, val accuracy: 0.101000\n",
      "Loss: inf, Train accuracy: 0.168778, val accuracy: 0.167000\n",
      "Loss: inf, Train accuracy: 0.132111, val accuracy: 0.129000\n",
      "Loss: inf, Train accuracy: 0.103000, val accuracy: 0.110000\n",
      "Loss: inf, Train accuracy: 0.114444, val accuracy: 0.136000\n",
      "Loss: inf, Train accuracy: 0.094222, val accuracy: 0.071000\n",
      "Loss: inf, Train accuracy: 0.079444, val accuracy: 0.078000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\src\\DL\\assignments\\assignment2\\layers.py:42: RuntimeWarning: overflow encountered in subtract\n",
      "  x -= np.max(x, axis=1, keepdims=True)\n",
      "c:\\users\\zabra\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "D:\\src\\DL\\assignments\\assignment2\\layers.py:42: RuntimeWarning: invalid value encountered in subtract\n",
      "  x -= np.max(x, axis=1, keepdims=True)\n",
      "D:\\src\\DL\\assignments\\assignment2\\layers.py:129: RuntimeWarning: invalid value encountered in greater\n",
      "  relu_grad = self.x > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-54a84f10cb84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m                       learning_rate_decay=learning_rate_decay)\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Params: lr={lr}, rs={rs}, opt={opt.__class__}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\src\\DL\\assignments\\assignment2\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[1;31m# the params\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m                 \u001b[1;31m# display(self.dataset.train_X[batch_indices])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss_and_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m                 \u001b[1;31m# raise Exception(\"Not implemented!\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\src\\DL\\assignments\\assignment2\\model.py\u001b[0m in \u001b[0;36mcompute_loss_and_gradients\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;31m# Hint: self.params() is useful again!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[0mreg_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ml2_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreg_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreg_grad\u001b[0m   \u001b[1;31m# raise Exception(\"Not implemented!\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\src\\DL\\assignments\\assignment2\\layers.py\u001b[0m in \u001b[0;36ml2_regularization\u001b[1;34m(W, reg_strength)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# TODO: implement l2 regularization and gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m# Your final implementation shouldn't have any loops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreg_strength\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mW\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mreg_strength\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zabra\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial)\u001b[0m\n\u001b[0;32m   2074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2075\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[1;32m-> 2076\u001b[1;33m                           initial=initial)\n\u001b[0m\u001b[0;32m   2077\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2078\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zabra\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "import itertools\n",
    "\n",
    "learning_rates = [5e-1, 1e-1, 5e-2, 1e-2]\n",
    "reg_strength = [1e-3, 1e-4, 1e-5]\n",
    "optimizers = [SGD(), MomentumSGD()]\n",
    "learning_rate_decay = 0.99\n",
    "hidden_layer_size = 128\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "for lr, rs, opt in itertools.product(learning_rates,\n",
    "                                     reg_strength,\n",
    "                                     optimizers):\n",
    "    model = TwoLayerNet(n_input=train_X.shape[1],\n",
    "                        n_output=10,\n",
    "                        hidden_layer_size=hidden_layer_size,\n",
    "                        reg=rs)\n",
    "    dataset = Dataset(train_X, train_y,\n",
    "                      val_X, val_y)\n",
    "    trainer = Trainer(model=model,\n",
    "                      dataset=dataset,\n",
    "                      optim=opt,\n",
    "                      num_epochs=num_epochs,\n",
    "                      batch_size=batch_size,\n",
    "                      learning_rate=lr,\n",
    "                      learning_rate_decay=learning_rate_decay)\n",
    "    \n",
    "    loss, train_acc, val_acc = trainer.fit()\n",
    "    \n",
    "    print(f'Params: lr={lr}, rs={rs}, opt={opt.__class__}')\n",
    "    print(f\"Loss: {loss[-1]:.4f}, \"\n",
    "          f\"Train accuracy: {train_acc[-1]:.4f}, \"\n",
    "          f\"val accuracy: {val_acc[-1]:.4f}\")\n",
    "    print()\n",
    "    \n",
    "    loss_history.append(loss[-1])\n",
    "    train_history.append(train_acc[-1])\n",
    "    val_history.append(val_acc[-1])\n",
    "    \n",
    "    if val_acc[-1] > best_val_accuracy:\n",
    "        best_val_accuracy = val_acc[-1]\n",
    "        best_classifier = model\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1aa039cc2b0>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural net test set accuracy: 0.765000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
